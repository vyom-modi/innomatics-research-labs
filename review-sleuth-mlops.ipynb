{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LZxtBHWmXIJ0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import Memory\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow.pyfunc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wUIWwNvKXKee"
   },
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/28 13:19:06 INFO mlflow.tracking.fluent: Experiment with name 'exp-6' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/583061336309648736', creation_time=1711612146289, experiment_id='583061336309648736', last_update_time=1711612146289, lifecycle_stage='active', name='exp-6', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(\"exp-6\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDsqoqoCXLU5",
    "outputId": "6c9df40a-de4e-4d0e-c258-522adbc00207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8518 entries, 0 to 8517\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Reviewer Name    8508 non-null   object \n",
      " 1   Review Title     8508 non-null   object \n",
      " 2   Place of Review  8468 non-null   object \n",
      " 3   Up Votes         8508 non-null   float64\n",
      " 4   Down Votes       8508 non-null   float64\n",
      " 5   Month            8053 non-null   object \n",
      " 6   Review text      8510 non-null   object \n",
      " 7   Ratings          8518 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 532.5+ KB\n",
      "None\n",
      "            Reviewer Name               Review Title  \\\n",
      "0            Kamal Suresh               Nice product   \n",
      "1       Flipkart Customer     Don't waste your money   \n",
      "2  A. S. Raja Srinivasan   Did not meet expectations   \n",
      "3     Suresh Narayanasamy                       Fair   \n",
      "4               ASHIK P A                Over priced   \n",
      "\n",
      "               Place of Review  Up Votes  Down Votes     Month  \\\n",
      "0   Certified Buyer, Chirakkal     889.0        64.0  Feb 2021   \n",
      "1   Certified Buyer, Hyderabad     109.0         6.0  Feb 2021   \n",
      "2  Certified Buyer, Dharmapuri      42.0         3.0  Apr 2021   \n",
      "3     Certified Buyer, Chennai      25.0         1.0       NaN   \n",
      "4                          NaN     147.0        24.0  Apr 2016   \n",
      "\n",
      "                                         Review text  Ratings  \n",
      "0  Nice product, good quality, but price is now r...        4  \n",
      "1  They didn't supplied Yonex Mavis 350. Outside ...        1  \n",
      "2  Worst product. Damaged shuttlecocks packed in ...        1  \n",
      "3  Quite O. K. , but nowadays  the quality of the...        3  \n",
      "4  Over pricedJust â?¹620 ..from retailer.I didn'...        1  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Display dataset information\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "9wBc7UrqXP7t",
    "outputId": "6317b348-d808-4d88-f6d1-91df86bb3169"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Review text</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice product</td>\n",
       "      <td>Nice product, good quality, but price is now r...</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Don't waste your money</td>\n",
       "      <td>They didn't supplied Yonex Mavis 350. Outside ...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did not meet expectations</td>\n",
       "      <td>Worst product. Damaged shuttlecocks packed in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fair</td>\n",
       "      <td>Quite O. K. , but nowadays  the quality of the...</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over priced</td>\n",
       "      <td>Over pricedJust â?¹620 ..from retailer.I didn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Review Title  \\\n",
       "0               Nice product   \n",
       "1     Don't waste your money   \n",
       "2  Did not meet expectations   \n",
       "3                       Fair   \n",
       "4                Over priced   \n",
       "\n",
       "                                         Review text  Ratings Sentiment  \n",
       "0  Nice product, good quality, but price is now r...        4  positive  \n",
       "1  They didn't supplied Yonex Mavis 350. Outside ...        1  negative  \n",
       "2  Worst product. Damaged shuttlecocks packed in ...        1  negative  \n",
       "3  Quite O. K. , but nowadays  the quality of the...        3  positive  \n",
       "4  Over pricedJust â?¹620 ..from retailer.I didn'...        1  negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select relevant columns\n",
    "df = df[[\"Review Title\", \"Review text\", \"Ratings\"]]\n",
    "\n",
    "# Preprocess the text and assign sentiment labels\n",
    "df['Sentiment'] = df['Ratings'].apply(lambda rating: 'negative' if rating <= 2 else 'positive')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3SaFOctXcwC",
    "outputId": "86626ffd-6ef3-453e-bf75-87f06fcb400f"
   },
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df[['Review text']]\n",
    "y = df['Sentiment']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess text function\n",
    "# nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    text = text.replace('READ MORE', '')\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r':\\)|:\\(|:\\D|:\\S', '', text)\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_text = [word for word in words if word not in stop_words]\n",
    "    filtered_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Apply preprocessing to training and test data\n",
    "X_train['clean_text'] = X_train['Review text'].apply(preprocess_text)\n",
    "X_test['clean_text'] = X_test['Review text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4qb7o_W1XpK1"
   },
   "outputs": [],
   "source": [
    "# Define models and pipelines\n",
    "pipelines = {\n",
    "    'naive_bayes': Pipeline([\n",
    "        ('vectorization', CountVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ]),\n",
    "    'decision_tree': Pipeline([\n",
    "        ('vectorization', CountVectorizer()),\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vectorization', CountVectorizer()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ]),\n",
    "    'random_forest': Pipeline([\n",
    "        ('vectorization', CountVectorizer()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    'svm': Pipeline([\n",
    "        ('vectorization', CountVectorizer()),\n",
    "        ('classifier', SVC())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'naive_bayes': [\n",
    "        {\n",
    "            'vectorization': [CountVectorizer()],\n",
    "            'vectorization__max_features' : [1000, 1500, 2000, 5000],\n",
    "            'classifier__alpha' : [1, 10]\n",
    "        }\n",
    "    ],\n",
    "    'decision_tree': [\n",
    "        {\n",
    "            'vectorization': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'vectorization__max_features' : [1000, 1500, 2000, 5000],\n",
    "            'classifier__max_depth': [None, 5, 10]\n",
    "        }\n",
    "    ],\n",
    "    'logistic_regression': [\n",
    "        {\n",
    "            'vectorization': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'vectorization__max_features' : [1000, 1500, 2000, 5000],\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "            'classifier__penalty': ['elasticnet'],\n",
    "            'classifier__l1_ratio': [0.4, 0.5, 0.6],\n",
    "            'classifier__solver': ['saga'],\n",
    "            'classifier__class_weight': ['balanced']\n",
    "        }\n",
    "    ],\n",
    "    'random_forest': [\n",
    "        {\n",
    "            'vectorization': [CountVectorizer()],\n",
    "            'vectorization__max_features' : [1000, 1500, 2000, 5000],\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 5, 10]\n",
    "        }\n",
    "    ],\n",
    "    'svm': [\n",
    "        {\n",
    "            'vectorization': [CountVectorizer(), TfidfVectorizer()],\n",
    "            'vectorization__max_features' : [1000, 1500, 2000, 5000],\n",
    "            'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFeD4EcXXz3u",
    "outputId": "390febfe-e00b-4123-de48-cef8b677cadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** naive_bayes **********\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Train F1 Score: 0.9579439252336448\n",
      "Best Test F1 Score: 0.9572649572649573\n",
      "Best Train Accuracy: 0.9260346345758732\n",
      "Best Test Accuracy: 0.9237089201877934\n",
      "********** decision_tree **********\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Train F1 Score: 0.9841508174841508\n",
      "Best Test F1 Score: 0.9459728206827974\n",
      "Best Train Accuracy: 0.9721162312885236\n",
      "Best Test Accuracy: 0.9043427230046949\n",
      "********** logistic_regression **********\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Train F1 Score: 0.9323818640675311\n",
      "Best Test F1 Score: 0.934201507882111\n",
      "Best Train Accuracy: 0.8859700616378046\n",
      "Best Test Accuracy: 0.8873239436619719\n",
      "********** random_forest **********\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Train F1 Score: 0.9840899625156185\n",
      "Best Test F1 Score: 0.955078125\n",
      "Best Train Accuracy: 0.9719694746110948\n",
      "Best Test Accuracy: 0.9190140845070423\n",
      "********** svm **********\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best Train F1 Score: 0.9570572296576073\n",
      "Best Test F1 Score: 0.9559870550161812\n",
      "Best Train Accuracy: 0.9232462577047256\n",
      "Best Test Accuracy: 0.92018779342723\n"
     ]
    }
   ],
   "source": [
    "# Perform GridSearchCV for each algorithm\n",
    "for algo in pipelines.keys():\n",
    "    # Start main run for the model\n",
    "    with mlflow.start_run(run_name=algo):\n",
    "        print(\"*\"*10, algo, \"*\"*10)\n",
    "        grid_search = GridSearchCV(estimator=pipelines[algo],\n",
    "                                   param_grid=param_grids[algo],\n",
    "                                   cv=5,\n",
    "                                   scoring='f1',\n",
    "                                   return_train_score=True,\n",
    "                                   verbose=1\n",
    "                                  )\n",
    "\n",
    "        grid_search.fit(X_train['clean_text'], y_train)\n",
    "\n",
    "        # Log parameters for the main run\n",
    "        best_params = grid_search.best_params_\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Log metrics for the main run\n",
    "        best_model = grid_search.best_estimator_\n",
    "        train_f1 = f1_score(y_train, best_model.predict(X_train['clean_text']), pos_label='positive')\n",
    "        test_f1 = f1_score(y_test, best_model.predict(X_test['clean_text']), pos_label='positive')\n",
    "        train_accuracy = accuracy_score(y_train, best_model.predict(X_train['clean_text']))\n",
    "        test_accuracy = accuracy_score(y_test, best_model.predict(X_test['clean_text']))\n",
    "\n",
    "        mlflow.log_metric(\"train_f1\", train_f1)\n",
    "        mlflow.log_metric(\"test_f1\", test_f1)\n",
    "        mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "        print('Best Train F1 Score:', train_f1)\n",
    "        print('Best Test F1 Score:', test_f1)\n",
    "        print('Best Train Accuracy:', train_accuracy)\n",
    "        print('Best Test Accuracy:', test_accuracy)\n",
    "\n",
    "        # Log the model for the main run\n",
    "        mlflow.sklearn.log_model(best_model, \"model\")\n",
    "\n",
    "        # Log each individual run's details\n",
    "        for i, params in enumerate(grid_search.cv_results_['params']):\n",
    "            with mlflow.start_run(nested=True, run_name=f\"Run-{i+1}\"):\n",
    "                # Fit the pipeline with the current parameters\n",
    "                pipeline = Pipeline([\n",
    "                    ('vectorization', params['vectorization']),\n",
    "                    ('classifier', pipelines[algo]['classifier'])  # Use the classifier of the pipeline\n",
    "                ])\n",
    "                pipeline.set_params(**params)\n",
    "                pipeline.fit(X_train['clean_text'], y_train)\n",
    "\n",
    "                # Calculate metrics for the individual run\n",
    "                train_f1 = f1_score(y_train, pipeline.predict(X_train['clean_text']), pos_label='positive')\n",
    "                test_f1 = f1_score(y_test, pipeline.predict(X_test['clean_text']), pos_label='positive')\n",
    "                train_accuracy = accuracy_score(y_train, pipeline.predict(X_train['clean_text']))\n",
    "                test_accuracy = accuracy_score(y_test, pipeline.predict(X_test['clean_text']))\n",
    "\n",
    "                # Log parameters for the individual run\n",
    "                mlflow.log_params(params)\n",
    "\n",
    "                # Log metrics for the individual run\n",
    "                mlflow.log_metric(\"train_f1\", train_f1)\n",
    "                mlflow.log_metric(\"test_f1\", test_f1)\n",
    "                mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "                mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "                # Log the model for the individual run\n",
    "                mlflow.sklearn.log_model(pipeline, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Umc1ZpfQX385",
    "outputId": "2379b597-4c85-4c63-c92b-30800b6b4c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** naive_bayes **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|█████████████████████| 9/9 [00:00<00:00, 512.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 0.9572649572649573\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.59      0.64       199\n",
      "    positive       0.95      0.97      0.96      1505\n",
      "\n",
      "    accuracy                           0.92      1704\n",
      "   macro avg       0.83      0.78      0.80      1704\n",
      "weighted avg       0.92      0.92      0.92      1704\n",
      "\n",
      "Prediction Time: 0.0038089752197265625 seconds\n",
      "********** decision_tree **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|█████████████████████| 9/9 [00:00<00:00, 506.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 0.9459728206827974\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.57      0.58       199\n",
      "    positive       0.94      0.95      0.95      1505\n",
      "\n",
      "    accuracy                           0.90      1704\n",
      "   macro avg       0.77      0.76      0.76      1704\n",
      "weighted avg       0.90      0.90      0.90      1704\n",
      "\n",
      "Prediction Time: 0.0038459300994873047 seconds\n",
      "********** logistic_regression **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|█████████████████████| 9/9 [00:00<00:00, 502.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 0.934201507882111\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.75      0.61       199\n",
      "    positive       0.96      0.91      0.93      1505\n",
      "\n",
      "    accuracy                           0.89      1704\n",
      "   macro avg       0.74      0.83      0.77      1704\n",
      "weighted avg       0.91      0.89      0.90      1704\n",
      "\n",
      "Prediction Time: 0.003634214401245117 seconds\n",
      "********** random_forest **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|█████████████████████| 9/9 [00:00<00:00, 188.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 0.955078125\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.50      0.59       199\n",
      "    positive       0.94      0.97      0.96      1505\n",
      "\n",
      "    accuracy                           0.92      1704\n",
      "   macro avg       0.83      0.74      0.77      1704\n",
      "weighted avg       0.91      0.92      0.91      1704\n",
      "\n",
      "Prediction Time: 0.025712251663208008 seconds\n",
      "********** svm **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|█████████████████████| 9/9 [00:00<00:00, 455.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 0.9559870550161812\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.46      0.57       199\n",
      "    positive       0.93      0.98      0.96      1505\n",
      "\n",
      "    accuracy                           0.92      1704\n",
      "   macro avg       0.85      0.72      0.76      1704\n",
      "weighted avg       0.91      0.92      0.91      1704\n",
      "\n",
      "Prediction Time: 0.053421974182128906 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the models from MLflow\n",
    "for algo in pipelines.keys():\n",
    "    print(\"*\"*10, algo, \"*\"*10)\n",
    "\n",
    "    # Load model from MLflow\n",
    "    client = MlflowClient()\n",
    "    runs = client.search_runs(experiment_ids=[583061336309648736], filter_string=f\"tags.mlflow.runName = '{algo}'\")\n",
    "    run_id = runs[0].info.run_id\n",
    "    model = mlflow.sklearn.load_model(f\"runs:/{run_id}/model\")\n",
    "\n",
    "    # Measure prediction time\n",
    "    start_time = time.time()\n",
    "\n",
    "    y_test_pred = model.predict(X_test['clean_text'])\n",
    "\n",
    "    # Calculate prediction time\n",
    "    prediction_time = time.time() - start_time\n",
    "    \n",
    "    # Log prediction time to MLflow\n",
    "    mlflow.log_metric(\"prediction_time\", prediction_time)\n",
    "\n",
    "    # Calculate and print evaluation metrics\n",
    "    test_f1 = f1_score(y_test, y_test_pred, pos_label='positive')\n",
    "    classification_rep = classification_report(y_test, y_test_pred)\n",
    "    print(\"Test F1 Score:\", test_f1)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    print(\"Prediction Time:\", prediction_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiVClQd7YtAA",
    "outputId": "dbe00538-1622-4246-8621-9102ff4af5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['positive']\n"
     ]
    }
   ],
   "source": [
    "# # Sample prediction using a model\n",
    "# model = mlflow.sklearn.load_model(\"runs:/1b4fddbdaf3b43f58b38914759d2071b/model\")\n",
    "\n",
    "new_data = [\n",
    "    \"The Product is ridiculously awesome\"\n",
    "]\n",
    "\n",
    "new_data_clean = [preprocess_text(doc) for doc in new_data]\n",
    "\n",
    "prediction = model.predict(new_data_clean)\n",
    "\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
